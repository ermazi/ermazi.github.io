<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 二麻运维</title>
    <link>https://ermazi.com/posts/</link>
    <description>Recent content in Posts on 二麻运维</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 23 Mar 2019 01:58:22 +0800</lastBuildDate>
    
	<atom:link href="https://ermazi.com/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>关于k8s实战的小技巧</title>
      <link>https://ermazi.com/2019/%E5%85%B3%E4%BA%8Ek8s%E5%AE%9E%E6%88%98%E7%9A%84%E5%B0%8F%E6%8A%80%E5%B7%A7/</link>
      <pubDate>Sat, 23 Mar 2019 01:58:22 +0800</pubDate>
      
      <guid>https://ermazi.com/2019/%E5%85%B3%E4%BA%8Ek8s%E5%AE%9E%E6%88%98%E7%9A%84%E5%B0%8F%E6%8A%80%E5%B7%A7/</guid>
      <description>实践是检验配置的唯一标准
 1. 存储  主要就是etcd了，至少有两处需要etcd，apiserver和calico网络插件。也没啥注意事项，保证2N+1个节点即可，小规模N=1，大规模N=2或者N=3，N也是可同时挂的节点数量。
 1.1. 配置  这两处可以共用一套etcd集群，也可以分别使用，calico网络插件对etcd的更新较少，小集群可以与apiserver共用，大了还是分开吧，自行取舍。
 推荐开启证书双向认证，共包含1套CA以及三套证书，分别用于:
 etcd服务端 etcd集群通信 etcd客户端  如果偷懒，可以共用证书秘钥对，只要ca相同即可。
像用于etcd集群通信的证书只能使用IP，所以必须签发携带IP信息的证书，比较麻烦。
1.2. 维护 1.2.1. 增减节点 2N+1的N不是手动配置的，而是根据成员动态调整的，所以在添加member的时候，需要注意member的可用性。
1.2.2. 迁移节点 这也是常用的运维操作，比如故障节点迁移，比如单机房变多机房等，具体步骤如下：
 变更证书，支持新机器域名以及IP访问 （假设满足2N+1个节点的集群）停止待迁移原节点 拷贝数据到目标节点 检查参数，集群状态设置为existing 命令修改原节点对应的member信息peer-urls为新节点endpoint 启动目标节点etcd 确认状态  1.3. 监控  磁盘性能 选主/切主频率 网络性能 proposal以及本地提交性能 &amp;hellip;  1.4. 最后一点话 像这种需要满足“大多数”的分布式集群，高可用部署是个挑战。这里的高可用其实是相对的，可以是机器隔离，可以是机柜隔离，可以是机房隔离，可以是城市隔离&amp;hellip;主要看你需要干什么，更高的可用性意味着更糟糕的性能，在于取舍。
 物理机，有冗余电源，UPS等可用性保证 网络设备有主备或者双活 应用层面也有对应的可用性保证方法  基于一致性协议的，比如raft等 基于vip漂来漂去的keepalived等   而现在普遍认为，跨可用区的才算真正高可用。。。应该是云宣传导致的思维定式。
凡事都得量力而行，辩证看待！
1.4.1. 举例 比如某A云只有A和B可用区，为了满足ETCD的高可用，非要使用某U云的A可用区作为第三个节点的部署位置，而A--&amp;gt;C和B--&amp;gt;C都是通过VPN打通（非专线）。
想法是不错，三个用区，三实例，挂一个机房都不会影响集群状态。可是毕竟经过了公网，集群的性能完全交给了稳定性相对较差的公网。
2. 主节点  k8s的心脏，大脑，负责与etcd存储通信，负责控制与调度，主节点单独部署，不作为Master节点。</description>
    </item>
    
    <item>
      <title>k8s内置scheduler算法[译]</title>
      <link>https://ermazi.com/2019/k8s%E5%86%85%E7%BD%AEscheduler%E7%AE%97%E6%B3%95/</link>
      <pubDate>Sat, 16 Mar 2019 10:58:22 +0800</pubDate>
      
      <guid>https://ermazi.com/2019/k8s%E5%86%85%E7%BD%AEscheduler%E7%AE%97%E6%B3%95/</guid>
      <description>k8s中的调度算法 k8s调度器根据一组规则去调度未分配Pod到相应的节点上，包括两个阶段，第一个阶段是过滤节点；第二个阶段是寻找最佳节点。[1][k8s调度器算法文档]
过滤节点 The purpose of filtering the nodes is to filter out the nodes that do not meet certain requirements of the Pod. For example, if the free resource on a node (measured by the capacity minus the sum of the resource requests of all the Pods that already run on the node) is less than the Pod&amp;rsquo;s required resource, the node should not be considered in the ranking phase so it is filtered out.</description>
    </item>
    
    <item>
      <title>UCloud使用体验</title>
      <link>https://ermazi.com/2019/ucloud%E4%BD%BF%E7%94%A8%E4%BD%93%E9%AA%8C%E6%8A%A5%E5%91%8A/</link>
      <pubDate>Fri, 15 Mar 2019 01:40:22 +0800</pubDate>
      
      <guid>https://ermazi.com/2019/ucloud%E4%BD%BF%E7%94%A8%E4%BD%93%E9%AA%8C%E6%8A%A5%E5%91%8A/</guid>
      <description>ucloud体验一周，惊喜与惊吓不断
 ## 1. 不说产品
1.1. UHost 让我印象最深的就是定价，所谓的按量，就是按小时收费，对于包月收费分为两种:到月末以及包月，据说是7天之内随意退，已使用部分按照已使用小时按月平均收费，也就是说，如果你使用七天内，千万别开按量，直接按月。
另一个亮点算是备注吧，可以写点extra信息，UHost标签系统单一，只支持一个Tag，并且叫做业务组，只有值，无键。
UCloud的网络定制化很弱，能建立VPC，子网，但是三层无法添加路由，也就是说，你如果在一个VPC中划分多子网，并且部署了VPN等网关设备和办公室或者其他IDC打通，那么想要路由特定流量，就需要在所有UHost中配置路由！！是所有！！如果你想要VPC以外的地方通过VPN访问内网ULB，可是ULB配置不了回包路由啊，咋整？？？
 解：牺牲源IP穿透，在VPN等网关设备添加SNAT，进行地址伪装；或者DNAT。
 另外，UCloud的SDN网络貌似MTU值较小，只有13901454（更正）？？如果使用了类IPsec的携带额外封包，需要将MSS调小（之前以为1360够了，那是针对MTU为默认的，现在需要调整到1314以下，方可使用，不然会因为数据包无法分段，导致通信受阻）
硬件隔离组？类似于部署级，是个好东西，用于防止多个虚拟机跑到一台物理机上，增加了可用性。IsolationGroup最多添加7台实例，猜测是为了满足大多数理论而定的值，比如zk、kafka、etcd等，但是对于无状态应用本身，需要在实例外分配app，尽可能使相同应用在一个硬件隔离组中。现在只是同组横向隔离，如果再整个组间隔离就更好了。在我们k8s的使用场景下，就只能按照NodeLabel创建硬件隔离组了，然后使用应用反亲和进行调度了（或者可以将硬件隔离组作为节点label，参与节点亲和性调度）。UK8s本身并没有考虑到硬件隔离组。
再说说防火墙?丫的就是安全组，而他们非要称之为外网防火墙，搞笑的是，他们API的参数还是叫SecurityGroup，哈哈哈。这个就比较坑了，UCloud产品脑袋绝对有坑，原因有3:
 每个实例必须且只能绑定一个防火墙。 想要放开所谓的&amp;rdquo;全部&amp;rdquo;，就得将全部TCP，全部UDP，ICMP，GRE规则都添加上，一共四条；产品怎么想的？增加操作复杂度，从而减少因为放开全部导致的安全事故？；放开&amp;rdquo;全部&amp;rdquo;有那么难吗？ 菜单栏叫基本协议，里面是2中提到的4种，我想着既然有基本协议，就应该有非基本协议吧？找了半天，然而，失望了，并没有。那么遇到非基本协议咋整？解:抱歉我们的产品已经满足了大多数用户的需求，你们这种小众需求，我们无法支持  不过至少有一点是可以的：
 他们同一个外网防火墙可以跨VPC复用，并且所有在外网防火墙内的UHost均可畅通无阻。最新了解：ucloud的外网防火墙仅仅针对EIP有效，内网无效！！！震惊！！！所以Ucloud的VPC内部无任何访问限制~~
 这也是我为啥妥协的原因，在不需要接入非同一VPC机器作为k8s节点的情况下，是能满足需求的。
啥玩意儿？为啥？你在说啥？
笔者使用calico插件，使用CrossSubnet模式，对于同一子网，走路由；跨子网，走ipip隧道，而ipip协议不包含它防火墙所定义的基本协议中，自然就无法放行了。sao！
再说说UHost本身吧， 两种特性：网络增强和高磁盘IO型，前者高包转发率，后者无限制SSD本地磁盘。网络增强型必须CPU&amp;gt;4；高IO型必须外挂本地数据盘。
UHost不支持metadata，不支持cloudinit。本地磁盘类型系统盘，默认镜像系统盘大小，只能增加，不能减少，本地盘扩缩容需要重启实例，名曰改配，费时。
API接口简陋，文档非同步最新，比如隔离组参数，API文档中都没有。
SDK基本没啥人用，比如Go的ucloud-go-sdk才36个star，估计还都是自己人点的，sdk的风格基本抄的aws，所以用起来还算顺手，不过描述与行为不符，缺乏单元测试。
问题来了，既然不支持metadata，当你在UHost中迷路，想知道自己是谁怎么办呢？又没有与生俱来的metadata，也没有哪里注入了UHost相关的信息？
 解：唯一办法，开只读权限secret，先获取本实例私有IP，然后调用APi，DescribeInstance，翻页遍历，条件: PrivateIp == IP，他们的查询实例信息，并不支持Filter参数，所以只能将数据Load到本地，然后比对过滤。（使用goroutine，分别取，只要有一个满足即退出，用race这种方式，应该能优化不少时间，当然是在实例很多的情况下）
 别问我怎么知道的，我也想尽了各种办法，只有这一条路可走，最终用go写了个metadataq的工具，用于模拟metadata信息查询，每次查询得2秒以上，具体还得依赖于地区，网络等（ucloud全球api只有一个入口！！！）后来跟k8s小组的人聊，他们也是这么获取UHostID等metadata的，然后再申请各种k8s需要的资源，比如UDisk，比如ULB，真心苦了这帮开发了。
UHost就说这么多吧，像前端UI的坑啊，API和UI信息不一致的坑啊，像高磁盘IO实例UI限制必须挂载本地数据盘然而创建完可删的逻辑错误啊，再比如删除本地磁盘并没有删除/etc/fstab相关条目导致实例无法启动的白痴型BUG啊，再再比如控制台登录没法复制的体验问题啊，再再再比如做镜像必须关机，镜像与快照无关等等等问题，我就不一一吐槽了。
1.2. ULB  用过最难用的负载均衡之一，看其配置，能勾起当初与大拿详聊LVS-DR模式的回忆。
 首先UCloud的ULB使用的使他们基于DPDK自研的类LVS-DR模式的负载均衡。
这措辞，一点没问题，DPDK是工具包，类LVS-DR，只是参照，最终还是自己研发的。
反正别管那么多，你当LVS-DR模式看待就行了，原理机制一模一样，哈哈哈。
DR模式包流转过程：包进LVS，修改其MAC地址为RS节点MAC，然后重新丢回进行二层转发，到达RS节点后，由于lo绑定了VIP与LVS入口IP一样，包合法，in，处理，封包出去，源地址为VIP，目标地址为客户端IP，直接发送给客户端。
如果用LVS-DR模式做四层负载均衡，会有什么特点呢？（不考虑性能，性能自然是杠杠的，不过大多数场景无益）
 LVS本身不涉及端口，不代理，只做mac修改包转发，也就意味着监听端口与RS监听端口必须一致；如果不一致，需要在RS节点使用iptables处理，在PREROUTING链进行端口转发，不过不易于维护。 RS本身可以通过ULB访问自己哟，无需使用俩代理做跳板（算个特点） 除了ULB控制台点点点，RS节点需要额外添加网卡信息，如果RS节点作为多ULB后端，需要绑定多个别名回环口网卡 存在本地限制，凡是后端RS节点访问ULB，会直接本地处理，一旦该RS对应的服务出现异常，那么另外N-1个RS也无法提供给该RS服务  ULB看样子更专注外网，外网支持HTTP/TCP/UDP，而TCP既支持类DR的包转发模式，也支持代理模式（Haproxy实现）；而可怜的内网，只支持包转发模式。
1.3. UK8s  简单说说吧，毕竟还是聊得挺愉快的
 算是公测新产品吧，貌似做k8s的团队没多少人，据观察，按条罗列吧
 所有k8s组件使用hyperkube混合工具启动 master独立，只包含etcd、k8s三大组件以及uk8s-controller（ucloud cloud controller），不包括节点组件kubelet以及kube-proxy，不作为master节点 网络这块，他们本身就是SDN，自研cni，做到UHostIP与PodIP同网段也不算太难，看样子应该没有支持NetworkPolicy 存储这块，跟aliyun类似，使用flexvolume，调用自研工具跟UDisk联动 LoadBalancer模式与ULB联动，3,4,5的控制部分都在uk8s-controller中 提供kubectl的控制台，用于命令行操作 Uk8s管理界面比较简单，就是将各种资源分tag罗列 Docker这块应用不知是否做定制，配置这块关闭iptables，禁用默认bridge，使用overlay2等，这会导致DIND模式无法访问网络 部分参数无效，但无妨 版本很新，已经v1.</description>
    </item>
    
    <item>
      <title>谈谈在阿里云自建k8s集群</title>
      <link>https://ermazi.com/2019/%E8%B0%88%E8%B0%88%E5%9C%A8%E9%98%BF%E9%87%8C%E4%BA%91%E8%87%AA%E5%BB%BAk8s%E9%9B%86%E7%BE%A4/</link>
      <pubDate>Sun, 10 Mar 2019 01:40:34 +0800</pubDate>
      
      <guid>https://ermazi.com/2019/%E8%B0%88%E8%B0%88%E5%9C%A8%E9%98%BF%E9%87%8C%E4%BA%91%E8%87%AA%E5%BB%BAk8s%E9%9B%86%E7%BE%A4/</guid>
      <description>我只是一名小小的运维，追求的是够用，极简，稳定，源码最多用于排查问题，二次开发基本是不够格的，据观察大多数人也处于我这个层级（大神忽略此条）。
 有人问：阿里云不是提供现成的k8s集群吗，为啥还要自建？吃饱了撑？
非也非也，单纯说好与不好都是耍流氓，应由利弊权衡之：
先说利:
 阿里云容器团队精心打磨，bug修复，功能改进 为k8s结合阿里云提供工具、服务进行粘合 提供一键滚动升级k8s版本 提供VPC级别的网络插件 提供aliyun-cloud-controller，能够很好的结合阿里云资源，比如vpc，路由表、SLB等 提供aliyun-disk-controller，能够直接挂载云存储(flexvolume) 开发了个application的服务，能够管理部分资源的发布，比如蓝绿部署等  再说弊:
 早期功能少，界面丑陋，设计不合理 比如40个节点的限制（因为路由条目最多加40条？笑） 比如默认外网暴露apiserver（10.12修复的bug忘了？好歹给个选项啊） 早期功能体验贼差，比如利用控制台无法添加节点（怀疑是自动化流程bug），只能先自己先创建ecs实例，然后作为节点添加 早期不支持network-policy（什么现在支持了？杠精滚粗，当时就直接拿flannel，开发了个支持vpc的backend就作为产品了，即使是现在的terway，也是flannel+calico-policy的组合版本） 早期默认内置nginx-ingress，这就觉得有点考虑得太多了点了，属于溺爱了。 还有个诟病的，就是直接将kubernetes-dashboard直接硬生生地嵌入了阿里云容器管理后台，最最最关键的是，把最重要的全局搜索框给干掉了，偷笑:D  笔者胡说，这些有的已经不是这样子了，臭杠精，没看到是早期啊，18年5月份之前！！
 阿里云还有个恶心的就是，啥玩意儿都只开源第一版，就是最初的一版，应该是为了应付开源精神以及开源协议的，后面的代码都只在企业内部更新迭代。
 开始考虑如何自建吧（无详细步骤，只谈可行性） 参考kubeadm，先用kubeadm整一版，然后研究其参数配置，将用得着的剥离出来，然后再参考绝世好文kubernetes the hard way，一步步来罗，简单的一比。
谈谈和阿里云结合 首先最重要的就是节点名称，你必须使用&amp;lt;region-id&amp;gt;.&amp;lt;instance-id&amp;gt;的格式去override，去扒阿里云aliyun-disk-controller创世版本，挂载磁盘至实例，都是直接读取的节点名称，然后调用阿里云api挂载的(不知道是不是写存储插件的开发脑子有坑，这么死板)，然后就去找个可用的阿里云存储插件镜像，阿里云文档中有个1.9.7可用，另外可以直接开个最新的容器服务，看最新的镜像tag。
存储有了，再说说网络，可以使用flannel的阿里云backend插件，直接用上阿里云网络资源，但是受限于iptables的条目！！40条！！ 也可以使用calico，不过你只能用上ipip模式，更高性能的三层模式，由于阿里云实例之间无法直接路由无法启用，对于aws来说，关闭src/dst检测包过滤，可以使用三层模式。
 杠精：那不行，ipip overlay network，性能不行，单队列，延迟变大了，影响应用~~ 笔者：我去你****，社区文章看多了吧，谁不想使用三层模式，阿里云限制啊。再说，在网络性能足够好的条件下，又能有多少影响呢？与其在这里抠这性能，不如花点心思改变一下调用链路，优化应用啊 。等你应用优化到极致，再吹毛求疵吧。
 DNS么直接用CoreDNS，性能杠杠的，可扩展，另外有插件可以自动扩展，这类服务属于重中之重之特重要服务，请单独调度到独立节点运行。然后forward到PowerDNS，PowerDNS在forward到阿里云100.100的DNS。
网络说完了，说说服务暴露吧，Service资源有种类型是LoadBalancer，一般云厂商都会将其与自家的负载均衡产品结合，一旦监听到该类型资源，就创建/修改对应的负载均衡配置。在实际使用中，这种方式并不是太紧迫，我们可以workaround，使用NodePort，也可以在k8s集群内部增加四层负载均衡，反代之，然后在边缘节点暴露服务，然后监测变化，挑选固定且未占用端口，将其挂载到SLB上对外。对于http类型的，这就比较好办了，毕竟应用层有信息进行路由，我们使用traefik作为ingress-controller，调度至边缘节点，使用label或者ingress/class区分对内，对外，用途等，然后使用几个固定的四层SLB反代之。（社区已经有CRD版本的ingress，替代官方默认的ingress，既可以暴露四层，又可以暴露七层，自行搜索）
其实现在差不多就能用了，RAM该有的权限给上，多可用区么也支持，无状态的可直接使用，有状态的依赖于volume，而阿里云最新的disk-controller也已经支持了存储卷调度VolumeSchedule。
监控啥的，使用prometheus+thanos+kube-state-metrics即可，如果需要水平扩容HPA么，再增加个metrics-server（兼容heapster）
docker仓库么，使用harbor，k8s上部署也有现成的helm chart，稍微改改也能用，另外，新版harbor也支持helm chart repository了哟，基本N个部门组，再来一个base镜像组，外加一个翻墙组(万恶的GFW)，多区域么，就同步几个基础组。
k8s资源编排工具么，那就用helm罗，其实笔者更喜欢谷歌自家的ksonnet，带环境概念，更加富有层次，复用程度也更高，不过需要额外学一门DSL，它叫jsonnet。
再说CI/CD流程？
CI这块使用jenkins罗，只使用Jenkinsfile编写，基于gitops理念。
CD这块么，使用argocd工具，也是基于gitops的。
开发调试么，能力有限，只能使用openvpn连入k8s集群内部罗，如果你使用阿里云产品，那么可以直接访问pod ip，确实方便。
好了，先说到这里了。</description>
    </item>
    
    <item>
      <title>Go语言实战之项目说明</title>
      <link>https://ermazi.com/2019/go%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%E4%B9%8B%E9%A1%B9%E7%9B%AE%E8%AF%B4%E6%98%8E/</link>
      <pubDate>Sun, 10 Mar 2019 01:25:22 +0800</pubDate>
      
      <guid>https://ermazi.com/2019/go%E8%AF%AD%E8%A8%80%E5%AE%9E%E6%88%98%E4%B9%8B%E9%A1%B9%E7%9B%AE%E8%AF%B4%E6%98%8E/</guid>
      <description>  慕课网小鱼儿讲师的文章还是挺好的，能将一个知识点在实际生产中的细节讲清楚，当然，其中很多事个人习惯，比如喜欢把所有变量定义到func头部，再比如喜欢使用goto语法，这些都挺好。
 1. 项目结构 项目按照功能分，或者说程序的入口。公用的一些资源定义放到common包中，在项目中尽可能使用Golang原生的一些包，比如net/http,flag,command等。
配置这块提出来，使用json格式文件，然后initConfig启动时加载，在编写配置项的时候，使用无用字段进行配置说明:D，这个有点想法。。。
对于需要全局使用的资源，使用G_xxx定义包级别的变量，方便全局使用。
2. 项目组成部分 </description>
    </item>
    
  </channel>
</rss>