<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>k8s on 二麻运维</title>
    <link>https://ermazi.com/tags/k8s/</link>
    <description>Recent content in k8s on 二麻运维</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 23 Mar 2019 01:58:22 +0800</lastBuildDate>
    
	<atom:link href="https://ermazi.com/tags/k8s/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>关于k8s实战的小技巧</title>
      <link>https://ermazi.com/2019/%E5%85%B3%E4%BA%8Ek8s%E5%AE%9E%E6%88%98%E7%9A%84%E5%B0%8F%E6%8A%80%E5%B7%A7/</link>
      <pubDate>Sat, 23 Mar 2019 01:58:22 +0800</pubDate>
      
      <guid>https://ermazi.com/2019/%E5%85%B3%E4%BA%8Ek8s%E5%AE%9E%E6%88%98%E7%9A%84%E5%B0%8F%E6%8A%80%E5%B7%A7/</guid>
      <description>实践是检验配置的唯一标准
 1. 存储  主要就是etcd了，至少有两处需要etcd，apiserver和calico网络插件。也没啥注意事项，保证2N+1个节点即可，小规模N=1，大规模N=2或者N=3，N也是可同时挂的节点数量。
 1.1. 配置  这两处可以共用一套etcd集群，也可以分别使用，calico网络插件对etcd的更新较少，小集群可以与apiserver共用，大了还是分开吧，自行取舍。
 推荐开启证书双向认证，共包含1套CA以及三套证书，分别用于:
 etcd服务端 etcd集群通信 etcd客户端  如果偷懒，可以共用证书秘钥对，只要ca相同即可。
像用于etcd集群通信的证书只能使用IP，所以必须签发携带IP信息的证书，比较麻烦。
1.2. 维护 1.2.1. 增减节点 2N+1的N不是手动配置的，而是根据成员动态调整的，所以在添加member的时候，需要注意member的可用性。
1.2.2. 迁移节点 这也是常用的运维操作，比如故障节点迁移，比如单机房变多机房等，具体步骤如下：
 变更证书，支持新机器域名以及IP访问 （假设满足2N+1个节点的集群）停止待迁移原节点 拷贝数据到目标节点 检查参数，集群状态设置为existing 命令修改原节点对应的member信息peer-urls为新节点endpoint 启动目标节点etcd 确认状态  1.3. 监控  磁盘性能 选主/切主频率 网络性能 proposal以及本地提交性能 &amp;hellip;  1.4. 最后一点话 像这种需要满足“大多数”的分布式集群，高可用部署是个挑战。这里的高可用其实是相对的，可以是机器隔离，可以是机柜隔离，可以是机房隔离，可以是城市隔离&amp;hellip;主要看你需要干什么，更高的可用性意味着更糟糕的性能，在于取舍。
 物理机，有冗余电源，UPS等可用性保证 网络设备有主备或者双活 应用层面也有对应的可用性保证方法  基于一致性协议的，比如raft等 基于vip漂来漂去的keepalived等   而现在普遍认为，跨可用区的才算真正高可用。。。应该是云宣传导致的思维定式。
凡事都得量力而行，辩证看待！
1.4.1. 举例 比如某A云只有A和B可用区，为了满足ETCD的高可用，非要使用某U云的A可用区作为第三个节点的部署位置，而A--&amp;gt;C和B--&amp;gt;C都是通过VPN打通（非专线）。
想法是不错，三个用区，三实例，挂一个机房都不会影响集群状态。可是毕竟经过了公网，集群的性能完全交给了稳定性相对较差的公网。
2. 主节点  k8s的心脏，大脑，负责与etcd存储通信，负责控制与调度，主节点单独部署，不作为Master节点。</description>
    </item>
    
    <item>
      <title>k8s内置scheduler算法[译]</title>
      <link>https://ermazi.com/2019/k8s%E5%86%85%E7%BD%AEscheduler%E7%AE%97%E6%B3%95/</link>
      <pubDate>Sat, 16 Mar 2019 10:58:22 +0800</pubDate>
      
      <guid>https://ermazi.com/2019/k8s%E5%86%85%E7%BD%AEscheduler%E7%AE%97%E6%B3%95/</guid>
      <description>k8s中的调度算法 k8s调度器根据一组规则去调度未分配Pod到相应的节点上，包括两个阶段，第一个阶段是过滤节点；第二个阶段是寻找最佳节点。[1][k8s调度器算法文档]
过滤节点 The purpose of filtering the nodes is to filter out the nodes that do not meet certain requirements of the Pod. For example, if the free resource on a node (measured by the capacity minus the sum of the resource requests of all the Pods that already run on the node) is less than the Pod&amp;rsquo;s required resource, the node should not be considered in the ranking phase so it is filtered out.</description>
    </item>
    
  </channel>
</rss>